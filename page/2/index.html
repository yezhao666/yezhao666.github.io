<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-local" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/09/local/" class="article-date">
  <time class="dt-published" datetime="2023-06-09T04:55:06.000Z" itemprop="datePublished">2023-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/09/local/">Spark(local)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-local"><a href="#Spark-local" class="headerlink" title="Spark (local)"></a>Spark (local)</h1><h2 id="1-安装python环境到window上"><a href="#1-安装python环境到window上" class="headerlink" title="1.安装python环境到window上"></a>1.安装python环境到window上</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Windows系统配置Anaconda</span><br><span class="line">打开资料中提供的:Anaconda3-2021.05-Windows-x86_64.exe文件</span><br><span class="line">配置国内源加速网络下载</span><br><span class="line">打开C:\Users\用户名\.condarc文件</span><br><span class="line">将以下内容进行替换；</span><br><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>
<h2 id="2-安装Python环境需要安装到Linux"><a href="#2-安装Python环境需要安装到Linux" class="headerlink" title="2.安装Python环境需要安装到Linux"></a>2.安装Python环境需要安装到Linux</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建与你环境 Python，基于Python 3.8</span><br><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure>
<p><img src="/../images/1.jpg" alt="图1"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">切换到与你环境内</span><br><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure>
<p><img src="/../images/2.jpg" alt="图"></p>
<h2 id="3-解压下载到spark安装包"><a href="#3-解压下载到spark安装包" class="headerlink" title="3.解压下载到spark安装包"></a>3.解压下载到spark安装包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/3.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">配置环境变量</span><br><span class="line">#HADOOP_HOME</span><br><span class="line">export SPARK_HOME=/export/server/spark</span><br><span class="line"></span><br><span class="line">#PYSPARK_PYTHON</span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"></span><br><span class="line">#HADOOP_CONF_DIR</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"></span><br><span class="line">#ZOOKEEPER_HOME</span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_2</span><br><span class="line">重置环境变量</span><br><span class="line">Source /etc/profile</span><br><span class="line">PYSPARK_PYTHON和 JAVA_HOME 需要同样配置在: `/root/.bashrc`中</span><br></pre></td></tr></table></figure>
<p><img src="/../images/4.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上传文件到Linux服务器中</span><br><span class="line">解压文件</span><br><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/5.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">设置软链接:</span><br><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>
<p><img src="/../images/6.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进去pyspark</span><br></pre></td></tr></table></figure>
<p><img src="/../images/7.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试：</span><br></pre></td></tr></table></figure>
<p><img src="/../images/8.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">打开服务器4040：</span><br></pre></td></tr></table></figure>
<p><img src="/../images/9.jpg" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Jps查看：</span><br></pre></td></tr></table></figure>
<p><img src="/../images/10.jpg" alt="图"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/09/local/" data-id="clj3neoll000dpk78ej7ofw9o" data-title="Spark(local)" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-alone" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/09/alone/" class="article-date">
  <time class="dt-published" datetime="2023-06-09T04:35:50.000Z" itemprop="datePublished">2023-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/09/alone/">Spark(stand-alone)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-StandAlone环境部署"><a href="#Spark-StandAlone环境部署" class="headerlink" title="Spark StandAlone环境部署"></a>Spark StandAlone环境部署</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">进入到spark的配置文件目录中, cd  spark/conf</span><br><span class="line">配置workers文件:</span><br><span class="line">改名, 去掉后面的.template后缀</span><br><span class="line">mv workers.template workers</span><br></pre></td></tr></table></figure>
<p><img src="/../images/11.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 编辑worker文件</span><br><span class="line">vim workers</span><br><span class="line"># 将里面的localhost删除, 添加</span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br><span class="line">到workers文件内</span><br></pre></td></tr></table></figure>
<p><img src="/../images/12.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">配置spark-env.sh文件</span><br><span class="line"># 1. 改名</span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>
<p><img src="/../images/13.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 2. 编辑spark-env.sh, 在底部追加如下内容</span><br><span class="line">## 设置JAVA安装目录</span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">## 指定spark老大Master的IP和提交任务的通信端口</span><br><span class="line"># 告知Spark的master运行在哪个机器上</span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line"># 告知sparkmaster的通讯端口</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"># 告知spark master的 webui端口</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"># worker cpu可用核数</span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"># worker可用内存</span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"># worker的工作通讯地址</span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"># worker的 webui地址</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line">## 设置历史服务器</span><br><span class="line"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>
<p><img src="/../images/14.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在HDFS上创建程序运行历史记录存放的文件夹:</span><br><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br><span class="line">配置spark-defaults.conf文件</span><br><span class="line"># 1. 改名</span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure>
<p><img src="/../images/15.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 2. 修改内容</span><br><span class="line"># 开启spark的日期记录功能</span><br><span class="line">spark.eventLog.enabled 	true</span><br><span class="line"># 设置spark日志记录的路径</span><br><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br><span class="line"># 设置spark日志是否启动压缩</span><br><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure>
<p><img src="/../images/16.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">配置log4j.properties 文件 </span><br><span class="line">改名</span><br><span class="line">mv log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure>
<p><img src="/../images/17.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将Spark安装文件夹  分发到其它的服务器上</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br><span class="line">分别在node2，node3设置软连接：</span><br><span class="line">ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</span><br><span class="line">启动历史服务器</span><br><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
<p><img src="/../images/18.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">启动Spark的Master和Worker进程</span><br><span class="line"># 启动全部master和worker</span><br><span class="line">sbin/start-all.sh</span><br><span class="line"># 或者可以一个个启动:</span><br><span class="line"># 启动当前机器的master</span><br><span class="line">sbin/start-master.sh</span><br><span class="line"># 启动当前机器的worker</span><br><span class="line">sbin/start-worker.sh</span><br><span class="line"># 停止全部</span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"># 停止当前机器的master</span><br><span class="line">sbin/stop-master.sh</span><br><span class="line"># 停止当前机器的worker</span><br><span class="line">sbin/stop-worker.sh</span><br><span class="line">查看Master的WEB UI</span><br></pre></td></tr></table></figure>
<p><img src="/../images/19.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">连接到StandAlone集群</span><br><span class="line">bin/pyspark --master spark://node1:7077</span><br></pre></td></tr></table></figure>
<p><img src="/../images/20.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看历史服务器WEB UI</span><br><span class="line">输入node1:18080</span><br></pre></td></tr></table></figure>
<p><img src="/../images/21.png" alt="图"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/09/alone/" data-id="clj3neolf0004pk78gb496x7u" data-title="Spark(stand-alone)" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ha" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/09/ha/" class="article-date">
  <time class="dt-published" datetime="2023-06-09T04:35:48.000Z" itemprop="datePublished">2023-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/09/ha/">Spark(HA)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-StandAloneHA搭建"><a href="#Spark-StandAloneHA搭建" class="headerlink" title="Spark StandAloneHA搭建"></a>Spark StandAloneHA搭建</h1><h2 id="1-首先配置spark-env-sh"><a href="#1-首先配置spark-env-sh" class="headerlink" title="1.首先配置spark-env.sh"></a>1.首先配置spark-env.sh</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> 删除SPARK_MASTER_HOST=node1这么操作的含义是不指定Master方便我们后续切换Master并在后面增加下面内容指定zookeeper</span><br><span class="line"></span><br><span class="line">_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span><br><span class="line"></span><br><span class="line"># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span><br><span class="line"># 指定Zookeeper的连接地址</span><br><span class="line"># 指定在Zookeeper中注册临时节点的路径</span><br><span class="line"># 然后将spark分发到node2和node3中</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/1.png" alt="图"></p>
<h2 id="2-测试Spark-StandAloneHA"><a href="#2-测试Spark-StandAloneHA" class="headerlink" title="2.测试Spark StandAloneHA"></a>2.测试Spark StandAloneHA</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">先在node1上启动一个master和全部的woiker</span><br><span class="line">然后在node2上启动一个备用的maste</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/2.png" alt="图"><br><img src="/../sparkha/3.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如图node1的状态是alive node2的状态是standy</span><br><span class="line">提交一个任务到node1上的spark</span><br><span class="line">bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/4.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后另起一台node1杀死node1的master</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/5.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以看到断开连接，等待一段时间。</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/6.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发现程序执行成功代表我们的HA模式搭建成功。在node1的master出现问题崩溃掉的时候会自动切换到node2的备用master上使我们的程序不会崩溃。</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/09/ha/" data-id="clj3neolf0003pk789sc46dax" data-title="Spark(HA)" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-yarn" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/09/yarn/" class="article-date">
  <time class="dt-published" datetime="2023-06-09T04:35:46.000Z" itemprop="datePublished">2023-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/09/yarn/">Spark(Yarn)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-On-YARN-环境搭建部署"><a href="#Spark-On-YARN-环境搭建部署" class="headerlink" title="Spark On YARN 环境搭建部署"></a>Spark On YARN 环境搭建部署</h1><h2 id="1-配置spark-env-sh"><a href="#1-配置spark-env-sh" class="headerlink" title="1.配置spark-env.sh"></a>1.配置spark-env.sh</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Cd /export/server/spark/conf</span><br><span class="line">Vi spark-env.sh</span><br><span class="line">加入一下内容：</span><br><span class="line">## 设置JAVA安装目录</span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">## 指定spark老大Master的IP和提交任务的通信端口</span><br><span class="line"># 告知Spark的master运行在哪个机器上</span><br><span class="line"></span><br><span class="line"># 告知sparkmaster的通讯端口</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"># 告知spark master的 webui端口</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"># worker cpu可用核数</span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"># worker可用内存</span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"># worker的工作通讯地址</span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"># worker的 webui地址</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line">## 设置历史服务器</span><br><span class="line"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br><span class="line">SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span><br><span class="line"># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span><br><span class="line"># 指定Zookeeper的连接地址</span><br><span class="line"># 指定在Zookeeper中注册临时节点的路径</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/7.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">连接YARN</span><br><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/8.png" alt="图"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure>
<p><img src="/../sparkha/9.png" alt="图"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/09/yarn/" data-id="clj3neolk000bpk78fiajajyf" data-title="Spark(Yarn)" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Docker" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/09/Docker/" class="article-date">
  <time class="dt-published" datetime="2023-06-09T03:30:48.000Z" itemprop="datePublished">2023-06-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/09/Docker/">Docker</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="一-需要虚拟机联网，安装yum工具"><a href="#一-需要虚拟机联网，安装yum工具" class="headerlink" title="一.  需要虚拟机联网，安装yum工具"></a>一.  需要虚拟机联网，安装yum工具</h2><p><img src="/../Docker/1.png" alt="图 1"></p>
<h2 id="二-配置网卡转发"><a href="#二-配置网卡转发" class="headerlink" title="二.	配置网卡转发"></a>二.	配置网卡转发</h2><h3 id="1-docker必须安装在centos7平台，内核版本不低于3-10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能"><a href="#1-docker必须安装在centos7平台，内核版本不低于3-10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能" class="headerlink" title="1.docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能"></a>1.docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能</h3><h3 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h3><p><img src="/../Docker/2.png" alt="图 2"></p>
<h3 id="2-重新加载内核参数"><a href="#2-重新加载内核参数" class="headerlink" title="2.重新加载内核参数"></a>2.重新加载内核参数</h3><p><img src="/../Docker/3.png" alt="图 3"></p>
<h2 id="三-利用yum进行docker安装"><a href="#三-利用yum进行docker安装" class="headerlink" title="三.	利用yum进行docker安装"></a>三.	利用yum进行docker安装</h2><h3 id="提前配置好yum仓库"><a href="#提前配置好yum仓库" class="headerlink" title="提前配置好yum仓库"></a>提前配置好yum仓库</h3><h3 id="阿里云自带仓库-2-阿里云提供的docker专属repo仓库"><a href="#阿里云自带仓库-2-阿里云提供的docker专属repo仓库" class="headerlink" title="阿里云自带仓库 2.阿里云提供的docker专属repo仓库"></a>阿里云自带仓库 2.阿里云提供的docker专属repo仓库</h3><p><img src="/../Docker/4.png" alt="图 4"><br><img src="/../Docker/5.png" alt="图 5"></p>
<h3 id="更新yum缓存"><a href="#更新yum缓存" class="headerlink" title="更新yum缓存"></a>更新yum缓存</h3><p><img src="/../Docker/6.png" alt="图 6"></p>
<h3 id="可以直接yum安装docker了"><a href="#可以直接yum安装docker了" class="headerlink" title="可以直接yum安装docker了"></a>可以直接yum安装docker了</h3><h4 id="查看源中可用版本"><a href="#查看源中可用版本" class="headerlink" title="查看源中可用版本"></a>查看源中可用版本</h4><p><img src="/../Docker/7.png" alt="图 7"></p>
<h4 id="yum的安装"><a href="#yum的安装" class="headerlink" title="yum的安装"></a>yum的安装</h4><p><img src="/../Docker/8.png" alt="图 8"><br><img src="/../Docker/9.png" alt="图 9"></p>
<h4 id="查看docker版本，验证是否验证成功"><a href="#查看docker版本，验证是否验证成功" class="headerlink" title="查看docker版本，验证是否验证成功"></a>查看docker版本，验证是否验证成功</h4><p><img src="/../Docker/10.png" alt="图 10"></p>
<h4 id="如果要卸载"><a href="#如果要卸载" class="headerlink" title="如果要卸载"></a>如果要卸载</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum remove -y docker-ce-xxx</span><br></pre></td></tr></table></figure>

<h2 id="四-配置镜像加速器"><a href="#四-配置镜像加速器" class="headerlink" title="四.	配置镜像加速器"></a>四.	配置镜像加速器</h2><h3 id="用于加速镜像文件下载-选用阿里云镜像站"><a href="#用于加速镜像文件下载-选用阿里云镜像站" class="headerlink" title="用于加速镜像文件下载,选用阿里云镜像站"></a>用于加速镜像文件下载,选用阿里云镜像站</h3><p><img src="/../Docker/11.png" alt="图 11"></p>
<h2 id="五-启动docker"><a href="#五-启动docker" class="headerlink" title="五.	启动docker"></a>五.	启动docker</h2><h3 id="启动docker前，一定要关闭防火墙后！！"><a href="#启动docker前，一定要关闭防火墙后！！" class="headerlink" title="启动docker前，一定要关闭防火墙后！！"></a>启动docker前，一定要关闭防火墙后！！</h3><h3 id="关闭"><a href="#关闭" class="headerlink" title="关闭"></a>关闭</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br></pre></td></tr></table></figure>

<h3 id="禁止开机启动防火墙"><a href="#禁止开机启动防火墙" class="headerlink" title="禁止开机启动防火墙"></a>禁止开机启动防火墙</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h3 id="通过命令启动docker："><a href="#通过命令启动docker：" class="headerlink" title="通过命令启动docker："></a>通过命令启动docker：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#systemctl start docker  # 启动docker服务</span><br><span class="line">#systemctl stop docker  # 停止docker服务</span><br><span class="line">#systemctl restart docker  # 重启docker服务</span><br></pre></td></tr></table></figure>

<h3 id="我们使用如下命令进行docker启动"><a href="#我们使用如下命令进行docker启动" class="headerlink" title="我们使用如下命令进行docker启动"></a>我们使用如下命令进行docker启动</h3><h4 id="docker配置文件重新加载"><a href="#docker配置文件重新加载" class="headerlink" title="docker配置文件重新加载"></a>docker配置文件重新加载</h4><p><img src="/../Docker/12.png" alt="图 12"></p>
<h4 id="查看docker信息"><a href="#查看docker信息" class="headerlink" title="查看docker信息"></a>查看docker信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker info</span><br><span class="line">docker ps</span><br><span class="line">docker images</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/13.png" alt="图 13"><br><img src="/../Docker/14.png" alt="图 14"><br><img src="/../Docker/15.png" alt="图 15"><br><img src="/../Docker/16.png" alt="图 16"></p>
<h4 id="docker-client"><a href="#docker-client" class="headerlink" title="docker-client"></a>docker-client</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">which docker</span><br></pre></td></tr></table></figure>
<h4 id="docker-daemon"><a href="#docker-daemon" class="headerlink" title="docker daemon"></a>docker daemon</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux |grep docker</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/17.png" alt="图 17"></p>
<h4 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux|grep containerd</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/18.png" alt="图 18"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status containerd</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/19.png" alt="图 19"></p>
<h2 id="Docker初体验"><a href="#Docker初体验" class="headerlink" title="Docker初体验"></a>Docker初体验</h2><h3 id="1-查看本地的docker镜像有哪些"><a href="#1-查看本地的docker镜像有哪些" class="headerlink" title="1.查看本地的docker镜像有哪些"></a>1.查看本地的docker镜像有哪些</h3><p><img src="/../Docker/20.png" alt="图 20"></p>
<h3 id="2-可选择删除旧版本"><a href="#2-可选择删除旧版本" class="headerlink" title="2.可选择删除旧版本"></a>2.可选择删除旧版本</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi 镜像id</span><br></pre></td></tr></table></figure>

<h3 id="3-搜索一下远程仓库中的镜像文件是否存在"><a href="#3-搜索一下远程仓库中的镜像文件是否存在" class="headerlink" title="3.搜索一下远程仓库中的镜像文件是否存在"></a>3.搜索一下远程仓库中的镜像文件是否存在</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search nginx</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/21.png" alt="图 21"></p>
<h3 id="4-拉取，下载镜像"><a href="#4-拉取，下载镜像" class="headerlink" title="4.拉取，下载镜像"></a>4.拉取，下载镜像</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nginx</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/22.png" alt="图 22"></p>
<h3 id="5-再次查看镜像"><a href="#5-再次查看镜像" class="headerlink" title="5.再次查看镜像"></a>5.再次查看镜像</h3><p><img src="/../Docker/23.png" alt="图 23"></p>
<h3 id="6-运行镜像，运行出具体内容，在容器中就跑着一个nginx服务"><a href="#6-运行镜像，运行出具体内容，在容器中就跑着一个nginx服务" class="headerlink" title="6.运行镜像，运行出具体内容，在容器中就跑着一个nginx服务"></a>6.运行镜像，运行出具体内容，在容器中就跑着一个nginx服务</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run 参数 镜像的名字/id</span><br></pre></td></tr></table></figure>

<h4 id="d-后台运行容器"><a href="#d-后台运行容器" class="headerlink" title="-d 后台运行容器"></a>-d 后台运行容器</h4><h4 id="p-80-80-端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口"><a href="#p-80-80-端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口" class="headerlink" title="-p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口"></a>-p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口</h4><h4 id="返回一个容器的id"><a href="#返回一个容器的id" class="headerlink" title="返回一个容器的id"></a>返回一个容器的id</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 80:80 nginx</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/24.png" alt="图 24"></p>
<h3 id="7-查看容器是否在运行"><a href="#7-查看容器是否在运行" class="headerlink" title="7.查看容器是否在运行"></a>7.查看容器是否在运行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/25.png" alt="图 25"></p>
<h3 id="8-访问网站"><a href="#8-访问网站" class="headerlink" title="8.访问网站"></a>8.访问网站</h3><p><img src="/../Docker/26.png" alt="图 26"></p>
<h3 id="9-停止容器"><a href="#9-停止容器" class="headerlink" title="9.停止容器"></a>9.停止容器</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop 容器id</span><br></pre></td></tr></table></figure>
<p><img src="/../Docker/27.png" alt="图 27"></p>
<h3 id="10-恢复容器"><a href="#10-恢复容器" class="headerlink" title="10.恢复容器"></a>10.恢复容器</h3><p><img src="/../Docker/28.png" alt="图 28"></p>
<h2 id="用docker切换不同发行版本，内核都为宿主机内核"><a href="#用docker切换不同发行版本，内核都为宿主机内核" class="headerlink" title="用docker切换不同发行版本，内核都为宿主机内核"></a>用docker切换不同发行版本，内核都为宿主机内核</h2><h3 id="1-利用docker获取不同的发行版镜像"><a href="#1-利用docker获取不同的发行版镜像" class="headerlink" title="1.利用docker获取不同的发行版镜像"></a>1.利用docker获取不同的发行版镜像</h3><p><img src="/../Docker/29.png" alt="图 29"><br><img src="/../Docker/30.png" alt="图 30"></p>
<h3 id="2-确认当前宿主机的发行版"><a href="#2-确认当前宿主机的发行版" class="headerlink" title="2.确认当前宿主机的发行版"></a>2.确认当前宿主机的发行版</h3><p><img src="/../Docker/31.png" alt="图 31"></p>
<h3 id="3-运行centos-7-8-2003发行版本"><a href="#3-运行centos-7-8-2003发行版本" class="headerlink" title="3.运行centos:7.8.2003发行版本"></a>3.运行centos:7.8.2003发行版本</h3><p>运行容器，且进入容器内部</p>
<p>参数解释，-i 交互式命令操作 -t 开启一个终端 bash 进入容器后执行的命令<br><img src="/../Docker/32.png" alt="图 32"></p>
<p>一个完整的系统，是由linux的内核+发行版，才组成了一个可以使用的完整的系统</p>
<p>利用docker容器，可以获取不同的发行版镜像，然后基于该镜像，运行出各种容器去使用</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/09/Docker/" data-id="clj3neol70000pk78cthi5ze9" data-title="Docker" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>
</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/06/20/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Kafka环境配置</a>
          </li>
        
          <li>
            <a href="/2023/06/20/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/">Kafka命令行操作</a>
          </li>
        
          <li>
            <a href="/2023/06/20/kafka-eagle%E5%90%84%E9%A1%B9%E5%8A%9F%E8%83%BD/">kafka-eagle各项功能</a>
          </li>
        
          <li>
            <a href="/2023/06/20/zookeeper/">Zookeeper</a>
          </li>
        
          <li>
            <a href="/2023/06/20/sqoop/">Sqoop</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>